import streamlit as st
import google.generativeai as genai
import pandas as pd
import time
import os
import re
import requests
import hashlib
import hmac
import base64
from PIL import Image
import io
from google.api_core import exceptions

# ==========================================
# 0. é…ç½®ä¸å¤š Key åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="LxU æµ‹å“å·¥å‚ (ç»ˆæå…¼å®¹ç‰ˆ)", layout="wide")

raw_keys = st.secrets.get("GEMINI_API_KEY", "")
API_KEYS = [k.strip() for k in raw_keys.split(",") if k.strip()]
NAVER_API_KEY = st.secrets.get("API_KEY")
NAVER_SECRET_KEY = st.secrets.get("SECRET_KEY")
NAVER_CUSTOMER_ID = st.secrets.get("CUSTOMER_ID")

if not API_KEYS or not NAVER_API_KEY:
    st.error("âš ï¸ å¯†é’¥ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ Secretsã€‚")
    st.stop()

SECRET_KEY_BYTES = NAVER_SECRET_KEY.encode("utf-8")
NAVER_API_URL = "https://api.searchad.naver.com/keywordstool"

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·å‡½æ•° (æ–°å¢è‡ªåŠ¨å‹å›¾)
# ==========================================
def compress_image(uploaded_file):
    """æŠŠå¤§å›¾å‹ç¼©åˆ° 2MB ä»¥å†…ï¼Œé˜²æ­¢é¢åº¦ç¬é—´è€—å°½"""
    img = Image.open(uploaded_file)
    # å¦‚æœæ˜¯ RGBA (PNG)ï¼Œè½¬æˆ RGB ä»¥ä¾¿å­˜ä¸º JPG
    if img.mode in ("RGBA", "P"):
        img = img.convert("RGB")
    
    img_byte_arr = io.BytesIO()
    # åˆå§‹è´¨é‡ 80
    img.save(img_byte_arr, format='JPEG', quality=80, optimize=True)
    
    # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œç»§ç»­é™è´¨é‡
    if img_byte_arr.tell() > 2 * 1024 * 1024:
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format='JPEG', quality=50, optimize=True)
        
    return img_byte_arr.getvalue()

def safe_generate_content(content_payload):
    """å°è¯•å¤šç§æ¨¡å‹åç§°ï¼Œè§£å†³ 404 å…¼å®¹æ€§é—®é¢˜"""
    # æŒ‰ç…§å…¼å®¹æ€§ä»æ–°åˆ°æ—§æ’åº
    model_names = ["gemini-1.5-flash", "models/gemini-1.5-flash", "gemini-pro-vision"]
    
    for i, key in enumerate(API_KEYS):
        genai.configure(api_key=key)
        for m_name in model_names:
            try:
                model = genai.GenerativeModel(m_name)
                response = model.generate_content(content_payload)
                return response
            except (exceptions.NotFound, exceptions.InvalidArgument):
                continue # åå­—ä¸å¯¹ï¼Œæ¢ä¸‹ä¸€ä¸ªåå­—è¯•
            except exceptions.ResourceExhausted:
                st.warning(f"âš ï¸ Key {i+1} é¢åº¦å·²å¹²æ¶¸ï¼Œåˆ‡æ¢ Key ä¸­...")
                break # é¢åº¦æ²¡äº†ï¼Œç›´æ¥æ¢ä¸‹ä¸€ä¸ª Key
            except Exception as e:
                st.warning(f"âš ï¸ Key {i+1} ä½¿ç”¨ {m_name} æ—¶å‡ºé”™: {e}")
                continue
    return None

# Naver æŸ¥é‡å‡½æ•°
def fetch_naver_data(main_keywords, pb, st_text):
    all_rows = []
    for i, mk in enumerate(main_keywords, start=1):
        st_text.text(f"ğŸ“Š Naver æ‹“è¯ä¸­ [{i}/{len(main_keywords)}]: {mk}")
        pb.progress(i / len(main_keywords))
        try:
            ts = str(int(time.time() * 1000))
            headers = {
                "X-Timestamp": ts, "X-API-KEY": NAVER_API_KEY, 
                "X-Customer": NAVER_CUSTOMER_ID, 
                "X-Signature": base64.b64encode(hmac.new(SECRET_KEY_BYTES, f"{ts}.GET./keywordstool".encode("utf-8"), hashlib.sha256).digest()).decode("utf-8")
            }
            res = requests.get(NAVER_API_URL, headers=headers, params={"hintKeywords": mk.replace(" ", ""), "showDetail": 1})
            if res.status_code == 200:
                for item in res.json().get("keywordList", [])[:8]:
                    pc = int(str(item.get("monthlyPcQcCnt", 0)).replace("<", "").replace(",", "")) if item.get("monthlyPcQcCnt") else 0
                    all_rows.append({"Naverè¯": item.get("relKeyword", ""), "æœç´¢é‡": pc, "AIæºè¯": mk})
        except: pass
        time.sleep(0.5)
    return pd.DataFrame(all_rows).drop_duplicates(subset=["Naverè¯"]).sort_values(by="æœç´¢é‡", ascending=False) if all_rows else pd.DataFrame()

# ==========================================
# 2. æ ¸å¿ƒ Prompt
# ==========================================
PROMPT_STEP_1 = """
ä½ æ˜¯ä¸€ä¸ªç²¾é€šéŸ©å›½ Coupang è¿è¥çš„ SEO ä¸“å®¶ï¼Œå“ç‰Œåä¸º LxUã€‚ä½ çš„å›¢é˜Ÿåœ¨ä¸­å›½ï¼Œé™¤éŸ©æ–‡è¯å¤–ï¼Œæ‰€æœ‰åˆ†ææ–‡å­—å¿…é¡» 100% ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
ç¬¬ä¸€ï¼Œæ‰¾å‡º20ä¸ªéŸ©å›½æœç´¢å…³é”®è¯ï¼Œå¿…é¡»ä»¥ Markdown è¡¨æ ¼å½¢å¼è¾“å‡ºã€‚
ç¬¬äºŒï¼Œå°†æ‰€æœ‰è¯æ±‡æ€»å»é‡æ”¾åœ¨ [LXU_KEYWORDS_START] å’Œ [LXU_KEYWORDS_END] ä¹‹é—´ï¼Œæ¯è¡Œä¸€ä¸ªè¯ã€‚
"""

PROMPT_STEP_3 = """
åŸºäºä»¥ä¸‹æ•°æ®è¾“å‡ºç»ˆæç­–ç•¥ï¼Œå¿…é¡»åŒ…å«ä¸€ä¸ªæ±‡æ€»æ‰€æœ‰è¯çš„è¡¨æ ¼ï¼š
{market_data}
"""

# ==========================================
# 3. è¿è¡Œç•Œé¢
# ==========================================
st.title("ğŸš€ LxU è‡ªåŠ¨åŒ–å·¥å‚ (ç»ˆæå…¼å®¹ç‰ˆ)")

# ä¾§è¾¹æ æ¸…ç†
if st.sidebar.button("ğŸ—‘ï¸ æ¸…ç†äº‘ç«¯ç¼“å­˜"):
    for k in API_KEYS:
        try:
            genai.configure(api_key=k); [genai.delete_file(f.name) for f in genai.list_files()]
            st.sidebar.success(f"æ¸…ç†æˆåŠŸ")
        except: pass

file = st.file_uploader("ğŸ“¥ ä¸Šä¼ è¯¦æƒ…é¡µ (ä¼šè‡ªåŠ¨å‹ç¼©)", type=["png", "jpg", "jpeg"])

if file and st.button("å¼€å§‹å…¨è‡ªåŠ¨æµæ°´çº¿"):
    # è‡ªåŠ¨å‹å›¾
    with st.spinner("æ­£åœ¨æ™ºèƒ½å‹ç¼©å›¾ç‰‡ä»¥èŠ‚çœé¢åº¦..."):
        compressed_data = compress_image(file)
        
    with st.status("ğŸ” ç¬¬ä¸€æ­¥ï¼šAI æè¯...", expanded=True) as s1:
        # ä½¿ç”¨å‹ç¼©åçš„æ•°æ®ç›´æ¥ä¸Šä¼ 
        temp_file_name = f"temp_upload_{int(time.time())}.jpg"
        with open(temp_file_name, "wb") as f:
            f.write(compressed_data)
        
        gen_file = genai.upload_file(path=temp_file_name)
        while gen_file.state.name == "PROCESSING":
            time.sleep(2)
            gen_file = genai.get_file(gen_file.name)
        
        response = safe_generate_content([gen_file, PROMPT_STEP_1])
        if response:
            st.markdown(response.text)
            kw_match = re.search(r"\[LXU_KEYWORDS_START\](.*?)\[LXU_KEYWORDS_END\]", response.text, re.DOTALL)
            kw_list = [re.sub(r'[^ê°€-í£\s]', '', l).strip() for l in kw_match.group(1).split('\n') if l.strip()] if kw_match else []
            s1.update(label=f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ", state="complete")
        else:
            st.error("âŒ å°è¯•äº†æ‰€æœ‰æ¨¡å‹åç§°å’Œ Keyï¼Œå‡æ— æ³•è¿è¡Œã€‚è¯·æ£€æŸ¥ API çŠ¶æ€ã€‚")
            st.stop()

    with st.status("ğŸ“Š ç¬¬äºŒæ­¥ï¼šNaver æŸ¥é‡...", expanded=True) as s2:
        pb = st.progress(0); txt = st.empty()
        df_market = fetch_naver_data(kw_list, pb, txt)
        if not df_market.empty:
            st.dataframe(df_market)
            s2.update(label="âœ… ç¬¬äºŒæ­¥å®Œæˆ", state="complete")
        else: st.error("Naver æ— æ•°æ®"); st.stop()

    with st.status("ğŸ§  ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»ˆæç­–ç•¥...", expanded=True) as s3:
        market_csv = df_market.to_csv(index=False)
        res3 = safe_generate_content([gen_file, PROMPT_STEP_3.format(market_data=market_csv)])
        if res3:
            st.markdown(res3.text)
            s3.update(label="âœ… ç¬¬ä¸‰æ­¥å®Œæˆ", state="complete")
            
    os.remove(temp_file_name)
    try: genai.delete_file(gen_file.name)
    except: pass
