import streamlit as st
import google.generativeai as genai
import pandas as pd
import time
import os
import re
import requests
import hashlib
import hmac
import base64

# ==========================================
# 0. é¡µé¢ä¸Ž Secrets é…ç½®
# ==========================================
st.set_page_config(page_title="LxU æµ‹å“å·¥ä½œæµ (ç»ˆæžç¨³å¥ç‰ˆ)", layout="wide")

GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY")
NAVER_API_KEY = st.secrets.get("API_KEY")
NAVER_SECRET_KEY = st.secrets.get("SECRET_KEY")
NAVER_CUSTOMER_ID = st.secrets.get("CUSTOMER_ID")

if not all([GEMINI_API_KEY, NAVER_API_KEY, NAVER_SECRET_KEY, NAVER_CUSTOMER_ID]):
    st.error("âš ï¸ ç¼ºå°‘ API å¯†é’¥ï¼è¯·ç¡®ä¿ Secrets ä¸­é…ç½®äº†æ‰€æœ‰å¿…éœ€çš„ Keyã€‚")
    st.stop()

genai.configure(api_key=GEMINI_API_KEY)
SECRET_KEY_BYTES = NAVER_SECRET_KEY.encode("utf-8")
NAVER_API_URL = "https://api.searchad.naver.com/keywordstool"

# ==========================================
# 1. æ ¸å¿ƒæŒ‡ä»¤ (ç¬¬ä¸€æ­¥å¼ºåˆ¶è¡¨æ ¼è¾“å‡º)
# ==========================================
PROMPT_STEP_1 = """
ä½ æ˜¯ä¸€ä¸ªç²¾é€šéŸ©å›½ Coupang è¿è¥çš„ SEO ä¸“å®¶ï¼Œå“ç‰Œåä¸º LxUã€‚æ³¨æ„ï¼šä½ çš„æ•´ä¸ªè¿è¥å›¢é˜Ÿéƒ½åœ¨ä¸­å›½ï¼Œæ‰€ä»¥ä½ å¿…é¡»éµå®ˆä»¥ä¸‹æžå…¶ä¸¥æ ¼çš„ã€è¯­è¨€è¾“å‡ºéš”ç¦»è§„èŒƒã€‘ï¼š
1. æ‰€æœ‰çš„â€œåˆ†æžè¿‡ç¨‹â€ã€â€œç­–ç•¥è§£é‡Šâ€ã€â€œä½¿ç”¨åŽŸå› â€ã€â€œä¸»å›¾å»ºè®®â€ç­‰ä»»ä½•æ²Ÿé€šæè¿°æ€§è´¨çš„æ–‡å­—ï¼Œå¿…é¡» 100% ä½¿ç”¨ã€ç®€ä½“ä¸­æ–‡ã€‘ï¼ç»å¯¹ç¦æ­¢ä½¿ç”¨éŸ©æ–‡è§£é‡Šï¼
2. åªæœ‰â€œéŸ©æ–‡å…³é”®è¯æœ¬èº«â€ã€â€œéŸ©è¯­æ ‡é¢˜â€å’Œâ€œå•†å“å¥½è¯„çš„éŸ©æ–‡åŽŸæ–‡â€è¿™ä¸‰ä¸ªéƒ¨åˆ†å…è®¸å‡ºçŽ°éŸ©æ–‡ï¼Œä¸”å¿…é¡»å…¨éƒ¨é™„å¸¦å¯¹åº”çš„ã€ä¸­æ–‡ç¿»è¯‘ã€‘ã€‚

--- æ ¸å¿ƒä»»åŠ¡ ---
ç¬¬ä¸€ï¼Œæ‰¾å‡º20ä¸ªäº§å“å…³é”®è¯ã€‚
ã€å¼ºåˆ¶è¾“å‡ºæ ¼å¼ã€‘ï¼š
1. å¿…é¡»å°†è¿™20ä¸ªå…³é”®è¯ä»¥ Markdown è¡¨æ ¼å½¢å¼è¾“å‡ºï¼
è¡¨æ ¼éª¨æž¶ä¸¥æ ¼å¦‚ä¸‹ï¼š
| åºå· | éŸ©æ–‡å…³é”®è¯ | ä¸­æ–‡ç¿»è¯‘ | çº¯ä¸­æ–‡ç­–ç•¥è§£é‡Š |
|---|---|---|---|
| 1 | ... | ... | ... |
2. åœ¨è¡¨æ ¼ä¸‹æ–¹ï¼Œå•ç‹¬è¾“å‡ºä¸€æ¬¾çº¯éŸ©æ–‡ã€é€—å·éš”å¼€çš„ç‰ˆæœ¬ã€‚

ç¬¬äºŒï¼Œè¾“å‡ºå¹¿å‘Šåˆ†ç»„ï¼ˆæ ¸å¿ƒ/ç²¾å‡†é•¿å°¾/æ¡æ¼ï¼‰ã€‚
è¾“å‡ºæ ¼å¼ï¼šMarkdown è¡¨æ ¼å½¢å¼ï¼Œè¡¨å¤´ï¼šã€åºå· | å¹¿å‘Šç»„åˆ†ç±» | éŸ©æ–‡å…³é”®è¯ | ä¸­æ–‡ç¿»è¯‘ | ä¸­æ–‡ç­–ç•¥è§£é‡Š | é¢„ä¼°æµé‡ | ç›¸å…³æ€§è¯„åˆ†ã€‘ã€‚

ç¬¬ä¸‰è‡³ç¬¬ä¸ƒéƒ¨åˆ†ï¼ˆæ ‡é¢˜ã€å†…éƒ¨åç§°ã€å¥½è¯„è¡¨ã€æ±‡æ€»è¡¨ã€ä¸»å›¾å»ºè®®ï¼‰å‡æŒ‰è¦æ±‚æ‰§è¡Œï¼Œæ‰€æœ‰è§£é‡Šæ–‡å­—å¿…é¡»çº¯ä¸­æ–‡ã€‚

ã€ç¨‹åºè¯»å–ä¸“å±žæŒ‡ä»¤ã€‘ï¼š
å°†â€œç¬¬å…­éƒ¨åˆ†â€åŽ»é‡æ±‡æ€»å…³é”®è¯æ”¾åœ¨ [LXU_KEYWORDS_START] å’Œ [LXU_KEYWORDS_END] ä¹‹é—´ï¼Œæ¯è¡Œä¸€ä¸ªã€‚
"""

PROMPT_STEP_3 = """
ä½ æ˜¯ä¸€ä½éŸ©å›½ Coupang è·¨å¢ƒç”µå•†è¿è¥ä¸“å®¶ã€‚é™¤éŸ©è¯­å…³é”®è¯å¤–ï¼Œæ‰€æœ‰åˆ†æžç”¨çº¯ä¸­æ–‡ã€‚
åŸºäºŽä»¥ä¸‹ Naver æ•°æ®è¾“å‡ºç»ˆæžç­–ç•¥ï¼š
{market_data}

ç¬¬ä¸€æ­¥ï¼šå…¨ç»´åº¦åˆ†æž (è§†è§‰/ç—›ç‚¹) - çº¯ä¸­æ–‡ã€‚
ç¬¬äºŒæ­¥ï¼šè¾“å‡ºç»Ÿä¸€çš„ä»˜è´¹å¹¿å‘ŠæŠ•æ”¾ç­–ç•¥è¡¨æ ¼ã€‚
è¡¨å¤´ï¼š| åºå· | å¹¿å‘Šç»„åˆ†ç±» | éŸ©æ–‡å…³é”®è¯ | ä¸­æ–‡ç¿»è¯‘ | æœˆæ€»æœç´¢é‡ | ç«žäº‰åº¦ | æŽ¨èç­–ç•¥ä¸Žè¯´æ˜Ž |
ç¬¬ä¸‰æ­¥ï¼šå¦å®šå…³é”®è¯åˆ—è¡¨ (çº¯ä¸­æ–‡ç®€è¿°ç†ç”±)ã€‚
"""

# ==========================================
# 2. Naver æ•°æ®æŠ“å–å‡½æ•°
# ==========================================
def clean_for_api(keyword: str) -> str:
    return re.sub(r"\s+", "", keyword)

def make_signature(method: str, uri: str, timestamp: str) -> str:
    message = f"{timestamp}.{method}.{uri}".encode("utf-8")
    signature = hmac.new(SECRET_KEY_BYTES, message, hashlib.sha256).digest()
    return base64.b64encode(signature).decode("utf-8")

def normalize_count(raw):
    if isinstance(raw, int): return raw
    if isinstance(raw, str):
        s = raw.strip().replace(",", "")
        if s.startswith("<"): return 5
        return int(s) if s.isdigit() else 0
    return 0

def fetch_naver_data(main_keywords, pb, st_text):
    all_rows = []
    total = len(main_keywords)
    for i, mk in enumerate(main_keywords, start=1):
        st_text.text(f"ðŸ“Š Naver æŸ¥è¯¢ä¸­ [{i}/{total}]: {mk}")
        pb.progress(i / total)
        try:
            timestamp = str(int(time.time() * 1000))
            sig = make_signature("GET", "/keywordstool", timestamp)
            headers = {"X-Timestamp": timestamp, "X-API-KEY": NAVER_API_KEY, "X-Customer": NAVER_CUSTOMER_ID, "X-Signature": sig}
            res = requests.get(NAVER_API_URL, headers=headers, params={"hintKeywords": clean_for_api(mk), "showDetail": 1})
            if res.status_code == 200:
                data = res.json()
                for item in data.get("keywordList", [])[:8]: 
                    pc = normalize_count(item.get("monthlyPcQcCnt", 0))
                    mob = normalize_count(item.get("monthlyMobileQcCnt", 0))
                    all_rows.append({
                        "Naverå®žé™…æœç´¢è¯": item.get("relKeyword", ""),
                        "æœˆæ€»æœç´¢é‡": pc + mob,
                        "ç«žäº‰åº¦": item.get("compIdx", "-"),
                        "AIæº¯æº(åŽŸè¯)": mk
                    })
        except Exception: pass
        time.sleep(1)
    df = pd.DataFrame(all_rows)
    return df.drop_duplicates(subset=["Naverå®žé™…æœç´¢è¯"]).sort_values(by="æœˆæ€»æœç´¢é‡", ascending=False) if not df.empty else df

# ==========================================
# 3. ä¸»å·¥ä½œæµ
# ==========================================
st.title("âš¡ LxU è‡ªåŠ¨åŒ–æµ‹å“å·¥åŽ‚")

files = st.file_uploader("ðŸ“¥ ä¸Šä¼ äº§å“è¯¦æƒ…é¡µ", type=["pdf", "png", "jpg"], accept_multiple_files=True)

if files and st.button("ðŸš€ å¯åŠ¨å…¨è‡ªåŠ¨é—­çŽ¯"):
    # å»ºè®®ä½¿ç”¨å…¼å®¹æ€§æœ€å¥½çš„ 1.5-flash
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    for file in files:
        st.divider()
        st.header(f"ðŸ“¦ å¤„ç†äº§å“ï¼š{file.name}")
        temp_path = f"temp_{file.name}"
        with open(temp_path, "wb") as f: f.write(file.getbuffer())
        
        with st.status("ðŸ” ç¬¬ä¸€æ­¥ï¼šAI è¯†å›¾ä¸Žè¡¨æ ¼æè¯...", expanded=True) as s1:
            gen_file = genai.upload_file(path=temp_path)
            while gen_file.state.name == "PROCESSING": time.sleep(2); gen_file = genai.get_file(gen_file.name)
                
            res1 = model.generate_content([gen_file, PROMPT_STEP_1])
            with st.expander("æŸ¥çœ‹ç¬¬ä¸€æ­¥æŠ¥å‘Š", expanded=False): st.write(res1.text)
                
            match = re.search(r"\[LXU_KEYWORDS_START\](.*?)\[LXU_KEYWORDS_END\]", res1.text, re.DOTALL | re.IGNORECASE)
            kw_list = []
            if match:
                raw_block = match.group(1)
                for line in re.sub(r'[,ï¼Œ]', '\n', raw_block).split('\n'):
                    clean_word = re.sub(r'[^ê°€-íž£\s]', '', line).strip()
                    if clean_word: kw_list.append(re.sub(r'\s+', ' ', clean_word))
            
            if kw_list: s1.update(label=f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ", state="complete")
            else: s1.update(label="âŒ æå–å¤±è´¥", state="error"); continue

        with st.status("ðŸ“Š ç¬¬äºŒæ­¥ï¼šNaver æŸ¥é‡...", expanded=True) as s2:
            pb = st.progress(0); txt = st.empty()
            df_market = fetch_naver_data(kw_list, pb, txt)
            if not df_market.empty:
                st.dataframe(df_market)
                s2.update(label=f"âœ… ç¬¬äºŒæ­¥å®Œæˆ (ç›®æ ‡ï¼š{len(kw_list)} âž¡ï¸ è¡ç”Ÿï¼š{len(df_market)})", state="complete")
            else: s2.update(label="âŒ ç¬¬äºŒæ­¥æ— æ•°æ®", state="error"); continue 

        with st.status("ðŸ§  ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»ˆæžç­–ç•¥...", expanded=True) as s3:
            res3 = model.generate_content([gen_file, PROMPT_STEP_3.format(market_data=df_market.to_csv(index=False))])
            st.markdown("### ðŸ† LxU ç»ˆæžæµ‹å“ç­–ç•¥æŠ¥å‘Š")
            st.success(res3.text)
            s3.update(label="âœ… ç¬¬ä¸‰æ­¥å®Œæˆ", state="complete")

        os.remove(temp_path)
        try: genai.delete_file(gen_file.name)
        except: pass
