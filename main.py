import streamlit as st
import google.generativeai as genai
import pandas as pd
import time
import os
import re
import requests
import hashlib
import hmac
import base64

# ==========================================
# 1. é¡µé¢é…ç½®ä¸åå° Secrets æ£€æŸ¥
# ==========================================
st.set_page_config(page_title="LxU å…¨é“¾è·¯å†³ç­–ç³»ç»Ÿ", layout="wide")

GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY")
NAVER_API_KEY = st.secrets.get("API_KEY")
NAVER_SECRET_KEY = st.secrets.get("SECRET_KEY")
NAVER_CUSTOMER_ID = st.secrets.get("CUSTOMER_ID")

if not all([GEMINI_API_KEY, NAVER_API_KEY, NAVER_SECRET_KEY, NAVER_CUSTOMER_ID]):
    st.error("âš ï¸ ç¼ºå°‘ API å¯†é’¥ï¼è¯·ç¡®ä¿åœ¨ .streamlit/secrets.toml ä¸­é…ç½®äº†æ‰€æœ‰å¿…éœ€çš„ Keyã€‚")
    st.stop()

# åˆå§‹åŒ–é…ç½®
genai.configure(api_key=GEMINI_API_KEY)
SECRET_KEY_BYTES = NAVER_SECRET_KEY.encode("utf-8")
NAVER_API_URL = "https://api.searchad.naver.com/keywordstool"

# ==========================================
# 2. æ ¸å¿ƒå¤§æ¨¡å‹æŒ‡ä»¤ (Prompts)
# ==========================================
PROMPT_STEP_1 = """
ä½ æ˜¯ä¸€ä¸ªç²¾é€šéŸ©å›½ Coupang è¿è¥çš„ SEO ä¸“å®¶ï¼Œå“ç‰Œåä¸º LxUã€‚

ç¬¬ä¸€ï¼Œæˆ‘æ˜¯ä¸€ä¸ªåœ¨éŸ©å›½åšcoupangå¹³å°çš„è·¨å¢ƒç”µå•†å–å®¶ï¼Œè¿™æ˜¯æˆ‘çš„äº§å“è¯¦æƒ…é¡µï¼Œæˆ‘ç°åœ¨éœ€è¦åå°æ‰¾å‡º20ä¸ªäº§å“å…³é”®è¯è¾“å…¥åˆ°åå°ä»¥ä¾¿è®©å¹³å°å¿«é€Ÿå‡†ç¡®çš„ä¸ºæˆ‘çš„äº§å“æ‰“ä¸Šå‡†ç¡®çš„æ ‡ç­¾åŒ¹é…æµé‡ã€‚è¯·å¸®æˆ‘æ‰¾åˆ°æˆ–è€…æ¨æµ‹å‡ºè¿™äº›ç¬¦åˆæœ¬åœ°æœç´¢ä¹ æƒ¯çš„éŸ©æ–‡å…³é”®è¯ã€‚åœ¨åˆ†æäº§å“çš„åŒæ—¶ä¹Ÿç»¼åˆè€ƒè™‘æ¨èå•†å“ä¸­ç±»ä¼¼äº§å“çš„æ ‡é¢˜æŒ–æ˜å…³é”®è¯ï¼ˆéœ€è¦20ä¸ªåå°è®¾ç½®çš„å…³é”®è¯ï¼Œä¸åŒ…å«å“ç‰Œè¯ï¼‰
è¾“å‡ºè¦æ±‚ï¼š
1.ä¿ç•™ç«–ç‰ˆåºå·æ’åˆ—å¤–åŠ ç­–ç•¥è§£é‡Šçš„ç‰ˆæœ¬ï¼Œå«ç¿»è¯‘æ–‡ã€‚
2.è¿˜éœ€è¦è¾“å‡ºä¸€æ¬¾é€—å·éš”å¼€çš„ç‰ˆæœ¬æ–¹ä¾¿åœ¨coupangåå°å½•å…¥ã€‚

ç¬¬äºŒï¼Œæˆ‘æ˜¯ä¸€ä¸ªç²¾é“ºï¼Œæ¨å¹¿ä¾§ç‡ä¸ºå‰æœŸå°‘é‡è¿›è´§å¿«é€Ÿä»˜è´¹æ¨å¹¿æµ‹å“çš„å–å®¶ã€‚æ‰¾ç²¾å‡†é•¿å°¾è¯åšä»˜è´¹æ¨å¹¿ï¼ˆéœ€è¦ç²¾å‡†æµé‡è¯ï¼ŒæŒ‰ç›¸å…³æ€§æ’åˆ—å¹¶æ‰“åˆ†1-5ï¼‰ã€‚
å¹¿å‘Šç»„ä¸€ä¸ºã€æ ¸å¿ƒå‡ºå•è¯ã€‘ã€‚
å¹¿å‘Šç»„äºŒä¸ºã€ç²¾å‡†é•¿å°¾å…³é”®è¯ã€‘ï¼ˆå°½é‡æŒ–æ˜30ä¸ªå·¦å³ï¼ŒåŒ…å«ç¼©å†™å¦‚'ìŠ¤ë…'ã€è¯­åºé¢ å€’ã€åœºæ™¯è¯ã€å…³è”ç«å“å¦‚Daisoç­‰ï¼‰ã€‚
å¹¿å‘Šç»„ä¸‰ä¸ºã€é•¿å°¾æ¡æ¼ç»„å¹¿å‘Šè¯ã€‘ï¼ˆä½CPCã€è´­ä¹°æ„å‘å¼ºã€Low Trafficã€‚åŒ…å«é”™åˆ«å­—ã€ç¼©å†™ã€æ–¹è¨€ç­‰å˜ä½“ï¼‰ã€‚
è¾“å‡ºæ ¼å¼ï¼šExcelè¡¨æ ¼å½¢å¼ã€åºå· | éŸ©æ–‡å…³é”®è¯ | ä¸­æ–‡ç¿»è¯‘ | ç­–ç•¥ç±»å‹ | é¢„ä¼°æµé‡(High/Medium/Low) | ç›¸å…³æ€§è¯„åˆ†ã€‘ã€‚

ç¬¬ä¸‰ï¼Œç”Ÿæˆä¸€ä¸ªé«˜ç‚¹å‡»ç‡ (High CTR) æ ‡é¢˜æ–¹æ¡ˆï¼šå…¬å¼ [å“ç‰Œå] + [ç›´å‡»ç—›ç‚¹å½¢å®¹è¯] + [æ ¸å¿ƒå·®å¼‚åŒ–å–ç‚¹] + [æ ¸å¿ƒå¤§è¯] + [æ ¸å¿ƒå±æ€§/æè´¨] + [åœºæ™¯/åŠŸèƒ½]ã€‚20ä¸ªå­—ä»¥å†…ï¼Œç¬¦åˆéŸ©å›½äººå¯è¯»æ€§ã€‚

ç¬¬å››ï¼Œæä¾›ä¸€ä¸ªäº§å“éŸ©è¯­åç§°ç”¨äºå†…éƒ¨ç®¡ç†ã€‚

ç¬¬äº”ï¼ŒæŒ‰ç…§äº§å“å–ç‚¹æ’°å†™5æ¡å•†å“å¥½è¯„ï¼Œè¯­æ³•è‡ªç„¶ã€é£æ ¼å„å¼‚ï¼Œæœ¬åœŸåŒ–è¡¨è¾¾ï¼Œè¡¨æ ¼å½¢å¼æ’åˆ—ã€‚

ç¬¬å…­ï¼Œå°†ä¸Šè¿°ä¸‰ä¸ªå¹¿å‘Šç»„çš„æ‰€æœ‰å…³é”®è¯è¿›è¡Œå»é‡æ±‡æ€»ï¼Œå•åˆ—çºµå‘åˆ—è¡¨è¾“å‡ºè¡¨æ ¼ã€‚

ç¬¬ä¸ƒï¼ŒAI ä¸»å›¾ç”Ÿæˆå»ºè®®ï¼šåŸºäºåœºæ™¯è¯å»ºè®®èƒŒæ™¯å’Œæ„å›¾ï¼Œä¸»å›¾ä¸¥ç¦å¸¦æ–‡å­—ã€‚

ã€ç¨‹åºè¯»å–ä¸“å±æŒ‡ä»¤ - æåº¦é‡è¦ã€‘ï¼š
ä¸ºäº†æ–¹ä¾¿æˆ‘çš„ç³»ç»Ÿè‡ªåŠ¨æŠ“å–ï¼Œè¯·åŠ¡å¿…å°†â€œç¬¬å…­éƒ¨åˆ†â€çš„æœ€ç»ˆå»é‡æ±‡æ€»å…³é”®è¯ï¼Œæ”¾åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ ‡è®°ä¹‹é—´è¾“å‡ºï¼æ¯è¡Œåªå†™ä¸€ä¸ªéŸ©æ–‡å…³é”®è¯ï¼Œå°½é‡ä¸è¦å¸¦ä¸­æ–‡æˆ–åºå·ã€‚
[LXU_KEYWORDS_START]
(åœ¨è¿™é‡Œå¡«å…¥çº¯éŸ©æ–‡å…³é”®è¯)
[LXU_KEYWORDS_END]
"""

PROMPT_STEP_3 = """
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´å®æˆ˜ç»éªŒçš„éŸ©å›½ Coupang è·¨å¢ƒç”µå•†è¿è¥ä¸“å®¶ï¼Œç²¾é€šéŸ©è¯­è¯­ä¹‰åˆ†æã€VOCæŒ–æ˜ä»¥åŠâ€œç²¾é“ºå¿«é€Ÿæµ‹å“â€çš„é«˜ ROAS å¹¿å‘Šç­–ç•¥ã€‚æˆ‘ä»¬åšçš„æ˜¯éŸ©å›½ç”µå•†coupangå¹³å°ï¼Œä½†æˆ‘æ˜¯ä¸€ä¸ªä¸­å›½å–å®¶ï¼Œè¾“å‡ºæˆ‘èƒ½çœ‹æ‡‚çš„ç»“æœã€‚å…³é”®è¯ç›¸å…³å†…å®¹ä¸è¦ç¿»è¯‘è‹±æ–‡ï¼Œä¿æŒéŸ©æ–‡ï¼Œåªè¦æœ‰å¯¹åº”çš„ä¸­æ–‡ç¤ºæ„å³å¯ã€‚

**æ ¸å¿ƒä»»åŠ¡ï¼š**
ç”¨æˆ·æä¾›äº†â€œäº§å“è¯¦æƒ…é¡µåŸå›¾â€åŠâ€œNaverå…³é”®è¯å®¢è§‚æœç´¢é‡æ•°æ®â€ã€‚ä½ éœ€è¦åŸºäºäº§å“ç‰¹æ€§å’ŒçœŸå®æ•°æ®è¡¨ç°ï¼Œè¾“å‡ºç²¾å‡†å¹¿å‘Šåˆ†ç»„ã€å¦å®šè¯è¡¨ã€‚æ‰€æœ‰å…³äºå…³é”®è¯çš„åˆ†æä¸è¦å«æœ‰ LxU çš„å“ç‰Œè¯ã€‚

ä»¥ä¸‹æ˜¯åˆšæŠ“å–åˆ°çš„å®¢è§‚å¸‚åœºæœç´¢é‡æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰ï¼š
{market_data}

**ç¬¬ä¸€æ­¥ï¼šå…¨ç»´åº¦åˆ†æ (Deep Analysis)**
1. è§†è§‰ä¸å±æ€§è¯†åˆ«ï¼š åˆ†æè¯¦æƒ…é¡µä¿¡æ¯ï¼Œé”å®šæ ¸å¿ƒå±æ€§ï¼ˆæè´¨ã€å½¢çŠ¶ã€åŠŸèƒ½ã€åœºæ™¯ï¼‰ã€‚
2. ç—›ç‚¹æŒ–æ˜ï¼š ä»ç«å“å·®è¯„ä¸­æç‚¼ç”¨æˆ·ç—›ç‚¹ï¼ˆå¦‚ï¼šå™ªéŸ³å¤§ã€æ˜“ç”Ÿé”ˆï¼‰ã€‚
3. æ’é™¤é€»è¾‘å»ºç«‹ï¼š æ˜ç¡®â€œç»å¯¹ä¸ç›¸å…³â€çš„å±æ€§ï¼ˆå¦‚ï¼šäº§å“æ˜¯å¡‘æ–™ï¼Œæ’é™¤â€œä¸é”ˆé’¢â€ï¼‰ã€‚

**ç¬¬äºŒæ­¥ï¼šå…³é”®è¯æ¸…æ´—ä¸æ‰“åˆ† (Filtering & Scoring)**
åŸºäºæˆ‘æä¾›çš„æ•°æ®åˆ—è¡¨ï¼Œè¿›è¡Œä¸¥æ ¼ç­›é€‰ï¼š
1. ç›¸å…³æ€§æ‰“åˆ† (1-5åˆ†)ï¼š
   * 1-2åˆ† (ä¿ç•™)ï¼š æ ¸å¿ƒè¯åŠç²¾å‡†é•¿å°¾è¯ã€‚
   * 3åˆ† (ä¿ç•™)ï¼š å¼ºå…³è”åœºæ™¯æˆ–ç«å“è¯ï¼ˆå¯ç”¨äºæ¡æ¼ï¼‰ã€‚
   * 4-5åˆ† (å‰”é™¤/å¦å®š)ï¼š å®½æ³›å¤§è¯æˆ–å±æ€§é”™è¯¯çš„è¯ã€‚
2. æµé‡ä¸ç—›ç‚¹åŠ æƒï¼šä¼˜å…ˆä¿ç•™èƒ½è§£å†³â€œç—›ç‚¹â€çš„è¯ã€‚å‚è€ƒâ€œæ€»æœç´¢é‡â€ï¼Œä¿ç•™è™½ç„¶æµé‡å°ä½†æç²¾å‡†çš„é•¿å°¾è¯ã€‚

**ç¬¬ä¸‰æ­¥ï¼šè¾“å‡ºäºŒå¤§æ¨¡å— (Output Modules)**

**æ¨¡å—ä¸€ï¼šä»˜è´¹å¹¿å‘ŠæŠ•æ”¾ç­–ç•¥è¡¨** (è¯·ä»¥ Markdown è¡¨æ ¼è¾“å‡º)
* **å¹¿å‘Šç»„åˆ†ç±»ï¼š**
   * ã€æ ¸å¿ƒå‡ºå•è¯ã€‘ï¼šæµé‡è¾ƒå¤§ï¼Œå®Œå…¨åŒ¹é…ã€‚æµè§ˆé‡ä»é«˜åˆ°ä½æ’åˆ—ã€‚
   * ã€ç²¾å‡†é•¿å°¾è¯ã€‘ï¼šæ ¸å¿ƒè¯+å…·ä½“å±æ€§ã€‚æµè§ˆé‡ä»é«˜åˆ°ä½æ’åˆ—ã€‚
   * ã€æ¡æ¼ä¸ç—›ç‚¹ç»„ã€‘ï¼šé”™åˆ«å­—ã€å€’åºã€æ–¹è¨€ã€åœºæ™¯è¯ã€ç«å“è¯ã€‚
   * ç»™å…³é”®è¯æ ‡è®°åºå·ã€‚å•ç‹¬æˆåˆ—ã€‚æµè§ˆé‡ä»é«˜åˆ°ä½æ’åˆ—ã€‚
   * æŒ‰åˆ†ç±»åˆ¶ä½œä¸‰ç»„å…³é”®è¯ç»“æœåˆ—è¡¨ã€‚ä¿è¯å‡†ç¡®çš„æƒ…å†µä¸‹å°½é‡ä¿è¯å…³é”®è¯æ•°é‡ã€‚

**æ¨¡å—äºŒï¼šå¦å®šå…³é”®è¯åˆ—è¡¨ (Negative Keywords)**
*ç”¨äºå¹¿å‘Šåå°å±è”½ï¼Œé˜²æ­¢æ— æ•ˆçƒ§é’±ã€‚*
* **å»ºè®®å±è”½çš„è¯ï¼š** `[è¯1], [è¯2], [è¯3]...`
* **å±è”½åŸå› ï¼š** [ç®€è¿°ï¼Œä¾‹å¦‚ï¼šæè´¨ä¸ç¬¦ã€åœºæ™¯é”™è¯¯ç­‰]
"""

# ==========================================
# 3. Naver æ•°æ®æŠ“å–å¼•æ“
# ==========================================
def clean_for_api(keyword: str) -> str:
    return re.sub(r"\s+", "", keyword)

def make_signature(method: str, uri: str, timestamp: str) -> str:
    message = f"{timestamp}.{method}.{uri}".encode("utf-8")
    signature = hmac.new(SECRET_KEY_BYTES, message, hashlib.sha256).digest()
    return base64.b64encode(signature).decode("utf-8")

def normalize_count(raw):
    if isinstance(raw, int): return raw
    if isinstance(raw, str):
        s = raw.strip()
        if s.startswith("<"): return 5
        if s.startswith(">"):
            num = s[1:].strip()
            return int(num) if num.isdigit() else 0
        s = s.replace(",", "")
        if s.isdigit(): return int(s)
    return 0

def fetch_naver_data(main_keywords, progress_bar, status_text):
    all_rows = []
    total = len(main_keywords)
    
    for i, mk in enumerate(main_keywords, start=1):
        status_text.text(f"ğŸ“Š æ­£åœ¨æŸ¥è¯¢ Naver æœç´¢é‡ [{i}/{total}]: {mk}")
        progress_bar.progress(i / total)
        query_kw = clean_for_api(mk)
        
        try:
            timestamp = str(int(time.time() * 1000))
            sig = make_signature("GET", "/keywordstool", timestamp)
            headers = {
                "X-Timestamp": timestamp, "X-API-KEY": NAVER_API_KEY,
                "X-Customer": NAVER_CUSTOMER_ID, "X-Signature": sig
            }
            res = requests.get(NAVER_API_URL, headers=headers, params={"hintKeywords": query_kw, "showDetail": 1})
            
            if res.status_code == 200:
                data = res.json()
                if "keywordList" in data:
                    # é™åˆ¶æŠ“å–æ·±åº¦ï¼Œé˜²æ­¢æ•°æ®æº¢å‡ºï¼Œä¸»è¯å–å‰ 8 ä¸ªå…³è”è¯
                    for item in data["keywordList"][:8]: 
                        pc = normalize_count(item.get("monthlyPcQcCnt", 0))
                        mob = normalize_count(item.get("monthlyMobileQcCnt", 0))
                        all_rows.append({
                            "æå–ä¸»è¯": mk,
                            "Naveræ‰©å±•è¯": item.get("relKeyword", ""),
                            "æ€»æœç´¢é‡(PC+Mob)": pc + mob,
                            "ç«äº‰åº¦": item.get("compIdx", "-")
                        })
        except Exception as e:
            pass
        time.sleep(1) # API ä¿æŠ¤é¢‘ç‡
        
    df = pd.DataFrame(all_rows)
    if not df.empty:
        df = df.drop_duplicates(subset=["Naveræ‰©å±•è¯"]).sort_values(by="æ€»æœç´¢é‡(PC+Mob)", ascending=False)
    return df

# ==========================================
# 4. ä¸» UI ä¸å·¥ä½œæµæ§åˆ¶
# ==========================================
st.title("âš¡ LxU è‡ªåŠ¨åŒ–æµ‹å“å·¥å‚ (ç»ˆæé—­ç¯ç‰ˆ)")
st.info("å·¥ä½œæµï¼šä¸Šä¼ è¯¦æƒ…é¡µ â¡ï¸ Geminiæå–åˆç­›è¯ â¡ï¸ NaverçœŸå®æœç´¢é‡å›æµ‹ â¡ï¸ Geminiç»ˆææ’å…µå¸ƒé˜µ")

files = st.file_uploader("ä¸Šä¼ äº§å“è¯¦æƒ…é¡µ (PDF/PNG/JPG)", type=["pdf", "png", "jpg"], accept_multiple_files=True)

if files and st.button("ğŸš€ å¯åŠ¨å…¨é“¾è·¯æç‚¼"):
    model = genai.GenerativeModel(model_name="gemini-2.5-flash") # å»ºè®®ç”¨ Flashï¼Œé€Ÿåº¦å¿«ä¸”ä¸æ˜“æŠ¥429
    
    for file in files:
        st.divider()
        st.header(f"ğŸ“¦ äº§å“å¤„ç†ä¸­ï¼š{file.name}")
        temp_path = f"temp_{file.name}"
        with open(temp_path, "wb") as f:
            f.write(file.getbuffer())
            
        # ------------------ ç¬¬ä¸€æ­¥ï¼šè¯†å›¾ä¸æå– ------------------
        with st.status("ğŸ” ç¬¬ä¸€æ­¥ï¼šå¤§æ¨¡å‹è§†è§‰æç‚¼åˆç­›è¯...", expanded=True) as s1:
            gen_file = genai.upload_file(path=temp_path)
            while gen_file.state.name == "PROCESSING":
                time.sleep(2)
                gen_file = genai.get_file(gen_file.name)
                
            res1 = model.generate_content([gen_file, PROMPT_STEP_1])
            
            with st.expander("ğŸ‘‰ ç‚¹å‡»æŸ¥çœ‹ï¼šç¬¬ä¸€æ­¥ AI åŸå§‹å…¨é‡è¾“å‡ºæŠ¥å‘Š"):
                st.write(res1.text)
            
            # --- ğŸš€ ä¿®å¤ç‰ˆï¼šæŒ‰è¡Œæå–ï¼Œä¿ç•™éŸ©æ–‡å†…éƒ¨çš„ç©ºæ ¼é•¿å°¾è¯ ---
            match = re.search(r"\[LXU_KEYWORDS_START\](.*?)\[LXU_KEYWORDS_END\]", res1.text, re.DOTALL | re.IGNORECASE)
            kw_list = []
            if match:
                raw_block = match.group(1)
                # å…¼å®¹å¤„ç†ï¼šæŠŠé€—å·å…¨éƒ¨æ›¿æ¢æˆæ¢è¡Œç¬¦
                raw_block = re.sub(r'[,ï¼Œ]', '\n', raw_block)
                
                # æŒ‰è¡Œåˆ†å‰²å¤„ç†
                for line in raw_block.split('\n'):
                    # åªä¿ç•™éŸ©æ–‡å’Œç©ºæ ¼ï¼Œæ´—æ‰æ•°å­—ã€æ ‡ç‚¹å’Œä¸­æ–‡
                    clean_word = re.sub(r'[^ê°€-í£\s]', '', line).strip()
                    # å‹ç¼©è¿ç»­ç©ºæ ¼
                    clean_word = re.sub(r'\s+', ' ', clean_word)
                    
                    if clean_word and clean_word not in kw_list:
                        kw_list.append(clean_word)
            else:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ç²¾å‡†é”šç‚¹ï¼Œå°è¯•ä»å…¨æ–‡æœ€åæŠ“å–...")
                tail_text = res1.text[-800:]
                for line in tail_text.split('\n'):
                    clean_word = re.sub(r'[^ê°€-í£\s]', '', line).strip()
                    clean_word = re.sub(r'\s+', ' ', clean_word)
                    if clean_word and clean_word not in kw_list:
                        kw_list.append(clean_word)
                kw_list = kw_list[:25]
                
            if kw_list:
                s1.update(label=f"âœ… ç¬¬ä¸€æ­¥å®Œæˆï¼æˆåŠŸæˆªè· {len(kw_list)} ä¸ªçº¯éŸ©æ–‡åˆç­›è¯ (åŒ…å«é•¿å°¾è¯)", state="complete")
                st.success(f"å‡†å¤‡å–‚ç»™ Naver çš„ç²¾å‡†è¯è¡¨ï¼š{kw_list}")
            else:
                s1.update(label="âŒ ç¬¬ä¸€æ­¥æå–å¤±è´¥ï¼Œæœªèƒ½æ‰¾åˆ°ä»»ä½•éŸ©æ–‡", state="error")
                continue # è·³è¿‡è¯¥æ–‡ä»¶

        # ------------------ ç¬¬äºŒæ­¥ï¼šNaver å›æµ‹ ------------------
        with st.status("ğŸ“Š ç¬¬äºŒæ­¥ï¼šè¿æ¥ Naver API è·å–å®¢è§‚æœç´¢é‡...", expanded=True) as s2:
            pb = st.progress(0)
            status_txt = st.empty()
            
            df_market = fetch_naver_data(kw_list, pb, status_txt)
            
            if not df_market.empty:
                st.dataframe(df_market) # å±•ç¤ºæœç´¢é‡è¡¨æ ¼
                s2.update(label="âœ… ç¬¬äºŒæ­¥å®Œæˆï¼çœŸå®å¸‚åœºæ•°æ®å·²è·å–", state="complete")
            else:
                s2.update(label="âŒ ç¬¬äºŒæ­¥å¤±è´¥ï¼ŒNaver æœªè¿”å›æœ‰æ•ˆæ•°æ®", state="error")
                st.error("è¯·æ£€æŸ¥å…³é”®è¯æ˜¯å¦è¿‡äºç”Ÿåƒ»ï¼Œæˆ– Naver API é¢åº¦æ˜¯å¦å—é™ã€‚")
                continue

        # ------------------ ç¬¬ä¸‰æ­¥ï¼šç»ˆæç­–ç•¥å†³ç­– ------------------
        with st.status("ğŸ§  ç¬¬ä¸‰æ­¥ï¼šAI å¤§è„‘ç»¼åˆå†³ç­– (åˆå¹¶æ•°æ®ä¸å›¾åƒ)...", expanded=True) as s3:
            # å°†ç¬¬äºŒæ­¥æŸ¥åˆ°çš„æ•°æ®è½¬æˆ CSV æ–‡æœ¬ï¼Œå–‚ç»™å¤§æ¨¡å‹
            market_csv_string = df_market.to_csv(index=False) 
            final_prompt = PROMPT_STEP_3.format(market_data=market_csv_string)
            
            # åŒé‡è¾“å…¥ï¼šPDF æ–‡ä»¶å¯¹è±¡ + å¸¦æœ‰çœŸå®æœç´¢é‡çš„ Prompt
            res3 = model.generate_content([gen_file, final_prompt])
            st.markdown("### ğŸ† ç»ˆæè¿è¥ç­–ç•¥æŠ¥å‘Š")
            st.success(res3.text)
            
            s3.update(label="âœ… ç¬¬ä¸‰æ­¥å®Œæˆï¼šç»ˆææ’å…µå¸ƒé˜µå·²ç”Ÿæˆ", state="complete")

        # ------------------ æ”¶å°¾ï¼šæ¸…ç†ä¸å¯¼å‡º ------------------
        os.remove(temp_path)
        try:
            genai.delete_file(gen_file.name) # æ¸…ç†äº‘ç«¯ç¼“å­˜
        except:
            pass
            
        final_report = f"ã€LxU äº§å“æµ‹å“æŠ¥å‘Šï¼š{file.name}ã€‘\n\n" + "="*40 + "\n[ç¬¬ä¸€æ­¥ AI åˆç­›åŸå§‹ç»“æœ]\n" + res1.text + "\n\n" + "="*40 + "\n[ç¬¬äºŒæ­¥ Naver çœŸå®æ•°æ®]\n" + market_csv_string + "\n\n" + "="*40 + "\n[ç¬¬ä¸‰æ­¥ ç»ˆæç­–ç•¥æ’å…µå¸ƒé˜µ]\n" + res3.text
        
        st.download_button(
            label=f"ğŸ“¥ ä¸€é”®ä¸‹è½½ {file.name} å®Œæ•´æµ‹å“æŠ¥å‘Š (TXT)", 
            data=final_report, 
            file_name=f"LxU_æµ‹å“å…¨è®°å½•_{file.name}.txt"
        )
